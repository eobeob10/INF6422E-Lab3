{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset Choice\n",
    "\n",
    "For this lab, we choose MNIST because:\n",
    "\n",
    "MNIST is small enough to train quickly, even on modest hardware.\n",
    "\n",
    "It is a common benchmark for adversarial attack research.\n",
    "\n",
    "Security concerns around digit recognition (e.g., check processing, secure entry systems) can illustrate the impact of adversarial attacks.\n",
    "\n",
    "Number of images used: We will use the standard MNIST dataset of 60,000 training images and 10,000 test images. From the 60,000 training images, we will create separate train, validation, and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Splits & Setup\n",
    "\n",
    "We will demonstrate two different splitting strategies:\n",
    "\n",
    "70/15/15 for training, validation, and testing\n",
    "\n",
    "60/20/20 for training, validation, and testing\n",
    "\n",
    "The PyTorch torchvision.datasets.MNIST dataset is already split into 60k training and 10k test images, so we will:\n",
    "\n",
    "Split the 60k “training” images into train and validation based on the desired ratio.\n",
    "\n",
    "Use the official 10k test images as our “testing” set.\n",
    "\n",
    "Alternatively, to strictly follow the instructions, you can shuffle the entire 70k images and do your own 70/15/15 or 60/20/20.\n",
    "\n",
    "Below is an example approach using the official 10k as a base test set, and adjusting the training split internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 0.01\n",
    "\n",
    "# MNIST dataset and transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # normalizing [0,1] data is optional since MNIST is grayscale (0 to 1),\n",
    "    # but we could do transforms.Normalize((0.1307,), (0.3081,)) if desired\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a helper function to create data samplers for 70/15/15 and 60/20/20\n",
    "def create_data_loaders(train_dataset, test_dataset, train_ratio, val_ratio):\n",
    "    \"\"\"\n",
    "    train_ratio + val_ratio + test_ratio = 1.0 (conceptually)\n",
    "    We use the official test_dataset as a separate test set.\n",
    "    \"\"\"\n",
    "    num_train = len(train_dataset)  # 60,000\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_train = int(np.floor(train_ratio * num_train))\n",
    "    split_val = int(np.floor((train_ratio + val_ratio) * num_train))\n",
    "\n",
    "    train_idx = indices[:split_train]\n",
    "    val_idx = indices[split_train:split_val]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    val_loader   = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for 70/15/15\n",
    "train_loader_70, val_loader_70, test_loader_70 = create_data_loaders(train_dataset, test_dataset,\n",
    "                                                                     train_ratio=0.70,\n",
    "                                                                     val_ratio=0.15)\n",
    "# Example for 60/20/20\n",
    "train_loader_60, val_loader_60, test_loader_60 = create_data_loaders(train_dataset, test_dataset,\n",
    "                                                                     train_ratio=0.60,\n",
    "                                                                     val_ratio=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Baseline CNN (2 Layers)\n",
    "\n",
    "3.1 Model Architecture <a name=\"model-architecture-2-layer\"></a>\n",
    "\n",
    "We will create a simple 2-layer CNN (two convolutional layers followed by fully connected layers). Per the assignment, we cannot use advanced external libraries for the CNN. We will use standard PyTorch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_2Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_2Layer, self).__init__()\n",
    "        # 1) First conv layer: from 1 channel (grayscale) to 16 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        # 2) Second conv layer: from 16 channels to 32 channels\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Flatten, then fully connected\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # after pooling\n",
    "        self.fc2 = nn.Linear(128, 10)          # 10 classes for MNIST\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution + ReLU + Pool\n",
    "        x = self.pool(self.relu(self.conv1(x)))  # -> (16, 14, 14)\n",
    "        x = self.pool(self.relu(self.conv2(x)))  # -> (32, 7, 7)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)                # -> 32*7*7\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Training & Validation <a name=\"training-and-validation-2-layer\"></a>\n",
    "\n",
    "Below is a generic training loop that we can reuse. We’ll define a helper function train_and_validate(model, train_loader, val_loader, epochs, lr) to train the model and return its best version based on validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    best_val_acc = 0.0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Track best weights\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on 70/15/15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 0.4708, Train Acc: 0.8518, Val Acc: 0.9613\n"
     ]
    }
   ],
   "source": [
    "model_2layer_70 = SimpleCNN_2Layer()\n",
    "model_2layer_70 = train_and_validate(model_2layer_70, train_loader_70, val_loader_70, epochs=EPOCHS, lr=LR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on 60/20/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 0.5076, Train Acc: 0.8343, Val Acc: 0.9606\n",
      "Epoch [2/5] Train Loss: 0.0991, Train Acc: 0.9697, Val Acc: 0.9741\n",
      "Epoch [3/5] Train Loss: 0.0665, Train Acc: 0.9792, Val Acc: 0.9799\n",
      "Epoch [4/5] Train Loss: 0.0494, Train Acc: 0.9849, Val Acc: 0.9778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_2layer_60 \u001b[38;5;241m=\u001b[39m SimpleCNN_2Layer()\n\u001b[1;32m----> 2\u001b[0m model_2layer_60 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_2layer_60\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_60\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader_60\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\poly\\OneDrive\\Documents\\GitHub\\INF6422E-Lab3\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\poly\\OneDrive\\Documents\\GitHub\\INF6422E-Lab3\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mSimpleCNN_2Layer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Convolution + ReLU + Pool\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))  \u001b[38;5;66;03m# -> (16, 14, 14)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# -> (32, 7, 7)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                \u001b[38;5;66;03m# -> 32*7*7\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\poly\\OneDrive\\Documents\\GitHub\\INF6422E-Lab3\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\poly\\OneDrive\\Documents\\GitHub\\INF6422E-Lab3\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\poly\\OneDrive\\Documents\\GitHub\\INF6422E-Lab3\\.conda\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\poly\\OneDrive\\Documents\\GitHub\\INF6422E-Lab3\\.conda\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_2layer_60 = SimpleCNN_2Layer()\n",
    "model_2layer_60 = train_and_validate(model_2layer_60, train_loader_60, val_loader_60, epochs=EPOCHS, lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Evaluation Metrics <a name=\"evaluation-metrics-2-layer\"></a>\n",
    "\n",
    "We compute accuracy, precision, recall, F1-score, and AUC-ROC. Note that AUC-ROC for multi-class can be computed in various ways (e.g., one-vs-rest). Below is a simple example using sklearn.metrics.roc_auc_score in a one-vs-rest manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(predicted.numpy())\n",
    "            all_probs.extend(probs.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='macro')\n",
    "    rec = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # For multi-class AUC, use one-vs-rest approach\n",
    "    # This requires a one-hot representation of the labels\n",
    "    all_labels_one_hot = np.zeros((len(all_labels), 10))\n",
    "    for i, label in enumerate(all_labels):\n",
    "        all_labels_one_hot[i, label] = 1\n",
    "    try:\n",
    "        auc_roc = roc_auc_score(all_labels_one_hot, np.array(all_probs), multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        # In case some class doesn't appear, fallback to None\n",
    "        auc_roc = None\n",
    "\n",
    "    return acc, prec, rec, f1, auc_roc\n",
    "\n",
    "# Evaluate the two models:\n",
    "print(\"=== Evaluation (70/15/15) ===\")\n",
    "acc_70, prec_70, rec_70, f1_70, auc_70 = evaluate_model(model_2layer_70, test_loader_70)\n",
    "print(f\"Accuracy: {acc_70:.4f}, Precision: {prec_70:.4f}, Recall: {rec_70:.4f}, F1: {f1_70:.4f}, AUC-ROC: {auc_70}\")\n",
    "\n",
    "print(\"\\n=== Evaluation (60/20/20) ===\")\n",
    "acc_60, prec_60, rec_60, f1_60, auc_60 = evaluate_model(model_2layer_60, test_loader_60)\n",
    "print(f\"Accuracy: {acc_60:.4f}, Precision: {prec_60:.4f}, Recall: {rec_60:.4f}, F1: {f1_60:.4f}, AUC-ROC: {auc_60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Comparison Table <a name=\"comparison-table\"></a>\n",
    "\n",
    "2-layer CNN (70/15/15):\n",
    "\n",
    "    Accuracy:\n",
    "\n",
    "    Precision:\n",
    "\n",
    "    Recall:\n",
    "\n",
    "    F1:\n",
    "\n",
    "    AUC-ROC:\n",
    "\n",
    "2-layer CNN (60/20/20):\n",
    "\n",
    "    Accuracy:\n",
    "\n",
    "    Precision:\n",
    "\n",
    "    Recall:\n",
    "\n",
    "    F1:\n",
    "\n",
    "    AUC-ROC:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bonus: 3-Layer CNN\n",
    "\n",
    "We extend the CNN to 3 convolutional layers. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_3Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_3Layer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(64*3*3, 128)  # because after 3 conv+pool, size is 3x3\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x))) # -> (16, 14, 14)\n",
    "        x = self.pool(self.relu(self.conv2(x))) # -> (32, 7, 7)\n",
    "        x = self.pool(self.relu(self.conv3(x))) # -> (64, 3, 3)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3layer_70 = SimpleCNN_3Layer()\n",
    "model_3layer_70 = train_and_validate(model_3layer_70, train_loader_70, val_loader_70, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "acc_3_70, prec_3_70, rec_3_70, f1_3_70, auc_3_70 = evaluate_model(model_3layer_70, test_loader_70)\n",
    "print(f\"3-Layer CNN (70/15/15) => Accuracy: {acc_3_70:.4f}, Precision: {prec_3_70:.4f}, Recall: {rec_3_70:.4f}, F1: {f1_3_70:.4f}, AUC-ROC: {auc_3_70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evasion Attacks (FGSM & PGD)\n",
    "\n",
    "We now generate adversarial examples to fool our trained model.\n",
    "\n",
    "5.1 Fast Gradient Sign Method (FGSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, images, labels, epsilon=0.2):\n",
    "    # Make images require gradient\n",
    "    images.requires_grad = True\n",
    "\n",
    "    outputs = model(images)\n",
    "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Collect sign of gradient\n",
    "    sign_data_grad = images.grad.data.sign()\n",
    "\n",
    "    # Create adversarial image\n",
    "    adv_images = images + epsilon * sign_data_grad\n",
    "    # Clip to [0,1] for valid image\n",
    "    adv_images = torch.clamp(adv_images, 0, 1)\n",
    "\n",
    "    return adv_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate adversarial examples on the test set and measure accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fgsm(model, device, data_loader, epsilon):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        adv_images = fgsm_attack(model, images, labels, epsilon=epsilon)\n",
    "        outputs = model(adv_images)\n",
    "        _, final_preds = torch.max(outputs, 1)\n",
    "        correct += (final_preds == labels).sum().item()\n",
    "\n",
    "        # Save some examples for visualization\n",
    "        adv_examples.append((images[0].cpu(), adv_images[0].cpu(), labels[0].cpu(), final_preds[0].cpu()))\n",
    "\n",
    "    final_acc = correct / len(data_loader.dataset)\n",
    "    return final_acc, adv_examples\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_2layer_70.to(device)\n",
    "\n",
    "epsilon = 0.2\n",
    "acc_fgsm, examples_fgsm = test_fgsm(model_2layer_70, device, test_loader_70, epsilon)\n",
    "print(f\"FGSM Attack with epsilon={epsilon}, Test Accuracy = {acc_fgsm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Projected Gradient Descent (PGD)\n",
    "\n",
    "PGD is an iterative version of FGSM. At each iteration, we do a small FGSM step and project back into the ϵ-ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, epsilon=0.2, alpha=0.01, iters=10):\n",
    "    ori_images = images.clone().detach()\n",
    "\n",
    "    for i in range(iters):\n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            adv_images = images + alpha * images.grad.sign()\n",
    "            # Projection: keep within epsilon-ball of original\n",
    "            eta = torch.clamp(adv_images - ori_images, min=-epsilon, max=epsilon)\n",
    "            images = torch.clamp(ori_images + eta, 0, 1).detach_()\n",
    "\n",
    "    return images\n",
    "\n",
    "def test_pgd(model, device, data_loader, epsilon, alpha=0.01, iters=10):\n",
    "    correct = 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        adv_images = pgd_attack(model, images, labels, epsilon=epsilon, alpha=alpha, iters=iters)\n",
    "        outputs = model(adv_images)\n",
    "        _, final_preds = torch.max(outputs, 1)\n",
    "        correct += (final_preds == labels).sum().item()\n",
    "    final_acc = correct / len(data_loader.dataset)\n",
    "    return final_acc\n",
    "\n",
    "acc_pgd = test_pgd(model_2layer_70, device, test_loader_70, epsilon=0.2, alpha=0.01, iters=10)\n",
    "print(f\"PGD Attack, Test Accuracy = {acc_pgd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Visualizing Adversarial Examples & Accuracy Drop\n",
    "\n",
    "How the images look before/after attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_adv_examples(examples, title=\"FGSM Examples\"):\n",
    "    plt.figure(figsize=(8,10))\n",
    "    for i in range(6):\n",
    "        original, adv, label, pred = examples[i]\n",
    "        plt.subplot(3,4,2*i+1)\n",
    "        plt.title(f\"Original (Label={label})\")\n",
    "        plt.imshow(original.squeeze(), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(3,4,2*i+2)\n",
    "        plt.title(f\"Adversarial (Pred={pred})\")\n",
    "        plt.imshow(adv.squeeze(), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "visualize_adv_examples(examples_fgsm, title=f\"FGSM e={epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe  the drop in accuracy from the clean model (e.g., ~98-99%) to the attacked model. Also, you can experiment with varying ϵ to see the trade-off between attack strength (lower accuracy) and detectability (bigger visual perturbations).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Data Poisoning Attacks\n",
    "\n",
    "Data poisoning modifies the training data to degrade or manipulate model behavior. An example is label flipping: choose a fraction of the training samples and flip their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_labels(dataset, poison_ratio=0.1, flip_from=0, flip_to=1):\n",
    "    \"\"\"\n",
    "    Flip 'flip_from' to 'flip_to' in 'poison_ratio' fraction of the dataset.\n",
    "    This is a simple example for demonstration.\n",
    "    \"\"\"\n",
    "    n_poison = int(len(dataset) * poison_ratio)\n",
    "    indices = np.random.choice(len(dataset), size=n_poison, replace=False)\n",
    "    for idx in indices:\n",
    "        img, label = dataset[idx]\n",
    "        if label == flip_from:\n",
    "            # Replace the label\n",
    "            dataset[idx] = (img, flip_to)\n",
    "\n",
    "# Poison 10% of training data: label \"0\" => label \"1\"\n",
    "train_dataset_poisoned = copy.deepcopy(train_dataset)\n",
    "poison_labels(train_dataset_poisoned, poison_ratio=0.1, flip_from=0, flip_to=1)\n",
    "\n",
    "# Now train a new model on the poisoned dataset (70/15/15 for example)\n",
    "train_loader_poisoned_70, val_loader_poisoned_70, test_loader_poisoned_70 = create_data_loaders(\n",
    "    train_dataset_poisoned, test_dataset, 0.70, 0.15\n",
    ")\n",
    "\n",
    "model_poisoned = SimpleCNN_2Layer()\n",
    "model_poisoned = train_and_validate(model_poisoned, train_loader_poisoned_70, val_loader_poisoned_70,\n",
    "                                    epochs=EPOCHS, lr=LR)\n",
    "\n",
    "acc_poisoned, prec_poisoned, rec_poisoned, f1_poisoned, auc_poisoned = evaluate_model(model_poisoned, test_loader_poisoned_70)\n",
    "print(f\"Poisoned Model => Accuracy: {acc_poisoned:.4f}, Precision: {prec_poisoned:.4f}, Recall: {rec_poisoned:.4f}, F1: {f1_poisoned:.4f}, AUC-ROC: {auc_poisoned}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe how the poisoning may degrade overall accuracy or specifically impact digit “0.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Defenses Against Adversarial Attacks\n",
    "\n",
    "7.1 Adversarial Training\n",
    "\n",
    "One approach is to retrain the model using a mixture of clean and adversarial examples. During each training iteration, generate adversarial examples (e.g., FGSM) on the fly and include them in the training batch. This helps the model learn robust features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training(model, train_loader, epochs=5, lr=0.01, epsilon=0.2):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            # Standard forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Now generate adversarial examples (FGSM)\n",
    "            images_adv = fgsm_attack(model, images, labels, epsilon=epsilon)\n",
    "            outputs_adv = model(images_adv)\n",
    "            loss_adv = criterion(outputs_adv, labels)\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss = loss + loss_adv\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "model_adv_trained = SimpleCNN_2Layer()\n",
    "model_adv_trained = adversarial_training(model_adv_trained, train_loader_70, epochs=EPOCHS, lr=LR, epsilon=0.2)\n",
    "\n",
    "# Evaluate against FGSM & PGD now\n",
    "acc_fgsm_adv, _ = test_fgsm(model_adv_trained, device, test_loader_70, epsilon=0.2)\n",
    "acc_pgd_adv = test_pgd(model_adv_trained, device, test_loader_70, epsilon=0.2, alpha=0.01, iters=10)\n",
    "print(f\"Adversarially Trained Model => FGSM Acc: {acc_fgsm_adv:.4f}, PGD Acc: {acc_pgd_adv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adversarially trained model is more robust to FGSM/PGD than the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 Feature Squeezing\n",
    "\n",
    "Feature squeezing reduces the complexity of inputs. Common approaches:\n",
    "\n",
    "    Bit depth reduction: E.g., round pixel values to fewer bits.\n",
    "    \n",
    "    Median filtering or other smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_bit_depth(img, bits=4):\n",
    "    # img in [0,1], multiply by 255, round, re-scale\n",
    "    scale_factor = 2 ** bits - 1\n",
    "    img = torch.round(img * scale_factor) / scale_factor\n",
    "    return img\n",
    "\n",
    "def feature_squeezing_inference(model, data_loader, bits=4):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            squeezed = reduce_bit_depth(images, bits)\n",
    "            outputs = model(squeezed)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "acc_squeezed = feature_squeezing_inference(model_2layer_70, test_loader_70, bits=4)\n",
    "print(f\"Feature Squeezing (4-bit) Accuracy: {acc_squeezed:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 Differential Privacy\n",
    "\n",
    "To protect against model inversion attacks, you can train with differential privacy, which injects noise into the gradients or parameters. A simple demonstration could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentially_private_training(model, train_loader, epochs=5, lr=0.01, noise_multiplier=1.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Add Gaussian noise to gradients for differential privacy\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    noise = torch.randn_like(param.grad) * noise_multiplier\n",
    "                    param.grad += noise\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"DP Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "model_dp = SimpleCNN_2Layer()\n",
    "model_dp = differentially_private_training(model_dp, train_loader_70, epochs=EPOCHS, lr=LR, noise_multiplier=0.1)\n",
    "\n",
    "dp_acc, dp_prec, dp_rec, dp_f1, dp_auc = evaluate_model(model_dp, test_loader_70)\n",
    "print(f\"Differentially Private Model => Accuracy: {dp_acc:.4f}, Precision: {dp_prec:.4f}, \"\n",
    "      f\"Recall: {dp_rec:.4f}, F1: {dp_f1:.4f}, AUC-ROC: {dp_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Conclusion\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "Chose the MNIST dataset (justification: widely used, easy to train, and relevant security implications).\n",
    "\n",
    "Implemented a 2-layer CNN baseline, evaluated on 70/15/15 and 60/20/20 splits, and compared performance metrics.\n",
    "\n",
    "Extended to a 3-layer CNN for a bonus point.\n",
    "\n",
    "Demonstrated evasion attacks (FGSM & PGD) and measured how accuracy drops with stronger attacks.\n",
    "\n",
    "Explored a data poisoning attack by flipping labels in part of the training set.\n",
    "\n",
    "Implemented and tested defense strategies: Adversarial Training, Feature Squeezing, and Differential Privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through these exercises, we observed:\n",
    "\n",
    "How small perturbations can severely degrade model performance in evasion attacks.\n",
    "\n",
    "How data poisoning can shift model behavior.\n",
    "\n",
    "How defenses can help, but often at a cost (e.g., performance or training complexity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
